{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from ipywidgets import SelectMultiple, FloatRangeSlider, interact, fixed\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from gerumo.visualization import metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_experiments_folder = '/mnt/storage-lite/experiments/cta/angular'\n",
    "\n",
    "experiments_include_patterns = [\n",
    "    'smooth_experiments/*_lst_*_l2*/evaluation/*/results.csv'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = defaultdict(dict)\n",
    "eval_datasets = set()\n",
    "for pattern in experiments_include_patterns:\n",
    "    for results_filepath in Path(root_experiments_folder).rglob(pattern):\n",
    "        *_, experiment_name, _, eval_dataset, _ = str(results_filepath).split('/') \n",
    "        eval_datasets.add(eval_dataset)\n",
    "        experiments[experiment_name][eval_dataset] = pd.read_csv(results_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(experiment_name, dataset):\n",
    "    alias = {\n",
    "        'test_g_cut1000': 'on-axis (cut)', \n",
    "        'test_gd_cut1000': 'off-axis (cut)',\n",
    "        'test_gd_full': 'off-axis'\n",
    "    }\n",
    "    return f'{experiment_name.split(\"_\", 3)[-1]} -  {alias[dataset]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca20123cf6f4e658bf40d9d81c21910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(SelectMultiple(description='Models & Ensembles', options=('20220604_072240_umonne_lst_cuâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(\n",
    "    experiments_names=SelectMultiple(\n",
    "        options=experiments.keys(),\n",
    "        rows=10, description='Models & Ensembles'),\n",
    "    datasets=SelectMultiple(\n",
    "        options=list(eval_datasets),\n",
    "        rows=4, description='Datasets'),\n",
    "    ylim=FloatRangeSlider(\n",
    "        value=[0, 1.0], min=0, max=2.0, step=0.1, description='Y limits:', continuous_update=False, orientation='horizontal',\n",
    "        readout=True, readout_format='.1f'\n",
    "    ),\n",
    "    xlim=FloatRangeSlider(\n",
    "        value=[-2, 2], min=-2.0, max=2.0, step=0.1, description='X limits:', continuous_update=False, orientation='horizontal',\n",
    "        readout=True, readout_format='.1f'\n",
    "    )\n",
    ")\n",
    "def plot_angular_comparison(experiments_names, datasets, ylim=(0, 2), xlim=(-2,2)):\n",
    "    if len(experiments_names) == 0: return\n",
    "    if len(datasets) == 0: return\n",
    "    log_xlim = (10**xlim[0], 10**xlim[1])\n",
    "    #fmts = [formats[model] for model in models]\n",
    "    #fmts = list(fmts)\n",
    "    global experiments\n",
    "    figure = plt.figure(figsize=(10, 10))\n",
    "    for experiment_name in experiments_names:\n",
    "        for dataset in datasets:\n",
    "            results = experiments[experiment_name].get(dataset, None)\n",
    "            if results is not None:\n",
    "                metrics.reconstruction_resolution(results, ['az', 'alt'], ylim=ylim, xlim=log_xlim, figure=figure, label=get_label(experiment_name, dataset))#, fmts=fmts)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 ('tf23')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ef67963dbb4be9d51d0b8f2d4d59e748d67c8cc5b776d08e4e371277ec333e32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
