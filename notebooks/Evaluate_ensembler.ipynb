{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53a64629",
   "metadata": {},
   "source": [
    "# Ensembler Evaluation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b32a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118a5643-f658-45f7-a47a-e92c88585526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53dc593-585f-4ddf-99ae-341de735fe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gerumo.data.dataset import describe_dataset\n",
    "from gerumo.data.generators import build_generator\n",
    "from gerumo.utils.engine import (\n",
    "    setup_cfg, setup_environment, setup_experiment, build_dataset\n",
    ")\n",
    "from gerumo.utils.structures import Event, Task\n",
    "from gerumo.models.base import build_ensembler\n",
    "from gerumo.visualization.samples import event_regression\n",
    "from gerumo.data.constants import TELESCOPES\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "args = dotdict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123ea728-5132-4903-b51e-9d6336606988",
   "metadata": {},
   "source": [
    "## Select experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1942e2a6-ae8e-411c-a178-f6c8e3725dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a trained model directory\n",
    "args['config_file'] = '/home/asuka/projects/gerumo2/config/regression/umonne/umonne_ensembler.yml'\n",
    "\n",
    "# Use the validation datasets for evaluation\n",
    "args['use_validation'] = True\n",
    "\n",
    "# Select a test datasets (on axis/off axis)\n",
    "args['dataset_name'] = None\n",
    "\n",
    "args['min_obs'] = 3\n",
    "\n",
    "args['opts'] = [\n",
    "    #'DATASETS.TEST.EVENTS', '',\n",
    "    #'DATASETS.TEST.TELESCOPES', ''\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8ba8b5-7136-41e6-a8d0-fed3d257eee8",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b8be2-bb60-4679-88f5-cf773cdf83bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configurations\n",
    "cfg = setup_cfg(args)\n",
    "evaluation_dir = setup_experiment(cfg, ensemble=True)\n",
    "logger = setup_environment(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b02df3-9f29-4fdb-8542-87d68be8ae2d",
   "metadata": {},
   "source": [
    "## Load evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3c99f7-9c0f-4b93-b852-ff839ac5a3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup evaluation datasets directory\n",
    "if args.use_validation:\n",
    "    evaluation_dataset_name = 'validation'\n",
    "else:\n",
    "    evaluation_dataset_name = 'test'\n",
    "if args.dataset_name is not None:\n",
    "    evaluation_subfolder = args.dataset_name\n",
    "else:\n",
    "    evaluation_subfolder = evaluation_dataset_name\n",
    "if args.min_obs > 1:\n",
    "    evaluation_subfolder += f'_min_obs_{args.min_obs}'\n",
    "evaluation_dir = evaluation_dir / evaluation_subfolder\n",
    "evaluation_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Build evaluation dataset\n",
    "evaluation_dataset = build_dataset(cfg, evaluation_dataset_name)\n",
    "# Filter by number of observation\n",
    "evaluation_dataset = evaluation_dataset[\n",
    "    evaluation_dataset.groupby('event_unique_id').event_id.transform('size') >= args['min_obs']\n",
    "]\n",
    "describe_dataset(evaluation_dataset, logger, save_to=evaluation_dir / 'description.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eb5966",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31018d8-563b-42a2-8192-cb4cf469ac9d",
   "metadata": {},
   "source": [
    "## Build generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b5bb8-2241-48c3-8acb-2d3cd1508b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_generator = build_generator(cfg, evaluation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4d63e8-781e-4e59-bae3-2275fa283c0b",
   "metadata": {},
   "source": [
    "## Load Ensembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6057160b-c459-4016-a964-1ef544f0cdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "input_shapes = evaluation_generator.get_input_shape()\n",
    "ensembler = build_ensembler(cfg, input_shapes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdad409a-296c-4d9b-89e5-a44117382be1",
   "metadata": {},
   "source": [
    "## Start evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d262bd-92f3-41e7-aa0f-41b285f7675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = []\n",
    "uncertainties = []\n",
    "for X, event_true in tqdm(evaluation_generator):\n",
    "    predictions, y_pred, uncertainty = ensembler(X, uncertainty=True)\n",
    "    events += Event.add_prediction_list(event_true, predictions, ensembler.task)\n",
    "    uncertainties += [u for u in uncertainty.numpy()]\n",
    "evaluation_results = Event.list_to_dataframe(events)\n",
    "evaluation_results['uncertainty'] = uncertainties\n",
    "evaluation_results.to_csv(evaluation_dir / 'results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986ade05",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1480eafb",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9310b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gerumo.visualization import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be284e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = pd.read_csv(evaluation_dir / 'results.csv')\n",
    "evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d32235",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if ensembler.task is Task.REGRESSION:\n",
    "    # Target Regression\n",
    "    targets = [t.split('_')[1] for t in cfg.OUTPUT.REGRESSION.TARGETS]\n",
    "    metrics.targets_regression(evaluation_results, targets)\n",
    "    # Resolution\n",
    "    metrics.reconstruction_resolution(evaluation_results, targets, ylim=(0, 2))\n",
    "    # Theta2 distribution\n",
    "    metrics.theta2_distribution(evaluation_results, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1518f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ensembler.task is Task.CLASSIFICATION:\n",
    "    # Classification Report\n",
    "    labels = evaluation_generator.output_mapper.classes\n",
    "    metrics.classification_report(evaluation_results.pred_class_id, evaluation_results.true_class_id, labels=labels)\n",
    "    metrics.confusion_matrix(evaluation_results.pred_class_id, evaluation_results.true_class_id, labels=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d76492",
   "metadata": {},
   "source": [
    "# Sample Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be54264",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d28b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select random batch\n",
    "batch_i = np.random.randint(len(evaluation_generator))\n",
    "X, event_true = evaluation_generator[batch_i]\n",
    "\n",
    "# Select samples from batch\n",
    "samples_j = np.random.randint(len(X), size=n_samples)\n",
    "X = [X[j] for j in samples_j]\n",
    "event_true = [event_true[j] for j in samples_j]\n",
    "\n",
    "# Prediction\n",
    "predictions, y, uncertainties = ensembler(X, uncertainty=True)\n",
    "event_predictions = Event.add_prediction_list(event_true, predictions, ensembler.task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05f9a7f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if ensembler.task is Task.REGRESSION:\n",
    "    pass\n",
    "    # output_type = ensembler.models[ensembler.telescopes[0]].REGRESSION_OUTPUT_TYPE\n",
    "    # targets = cfg.OUTPUT.REGRESSION.TARGETS\n",
    "    # targets_domains = cfg.OUTPUT.REGRESSION.TARGETS_DOMAINS\n",
    "    # for j in range(n_samples):\n",
    "    #     # Plot input\n",
    "    #     input_observation = X[j]\n",
    "    #     # Plot event prediction\n",
    "    #     event_prediction = event_predictions[j]\n",
    "    #     model_output = y[j]\n",
    "    #     event_regression(event_prediction, model_output, output_type, targets, targets_domains)\n",
    "    #     # Plot uncertainty\n",
    "    #     uncertainty = uncertainties[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c75895",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ensembler.task is Task.CLASSIFICATION:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aad85ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ef67963dbb4be9d51d0b8f2d4d59e748d67c8cc5b776d08e4e371277ec333e32"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
