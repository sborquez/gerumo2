{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53a64629",
   "metadata": {},
   "source": [
    "# Neural Network Evaluation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b32a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118a5643-f658-45f7-a47a-e92c88585526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53dc593-585f-4ddf-99ae-341de735fe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gerumo.data.dataset import describe_dataset\n",
    "from gerumo.data.generators import build_generator\n",
    "from gerumo.utils.engine import (\n",
    "    setup_cfg, setup_environment, setup_experiment, setup_model, load_model, build_dataset, get_dataset_name\n",
    ")\n",
    "from gerumo.utils.structures import Event, Task\n",
    "from gerumo.models.base import build_model\n",
    "from gerumo.visualization.metrics import training_history\n",
    "from gerumo.visualization.samples import event_regression\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "args = dotdict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123ea728-5132-4903-b51e-9d6336606988",
   "metadata": {},
   "source": [
    "## Select experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1942e2a6-ae8e-411c-a178-f6c8e3725dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a trained model directory\n",
    "#args['config_file'] = '/mnt/storage-lite/experiments/cta/angular/smooth_experiments/20220604_072240_umonne_lst_cut_l21000'\n",
    "args['config_file'] = '/mnt/storage-lite/experiments/cta/angular/smooth_experiments/20220604_072240_umonne_lst_full_l2100'\n",
    "#args['config_file'] = '/mnt/storage-lite/experiments/cta/angular/smooth_experiments/20220604_072240_umonne_lst_full_l210'\n",
    "#args['config_file'] = '/mnt/storage-lite/experiments/cta/angular/smooth_experiments/20220604_072240_umonne_lst_cut_l210'\n",
    "\n",
    "#args['config_file'] = '/home/asuka/projects/gerumo2/tools/output/20220329_094830_bmo_mst_regression'\n",
    "\n",
    "# Select the best batch\n",
    "args['epoch'] = -1\n",
    "\n",
    "# Use the validation datasets for evaluation\n",
    "args['use_validation'] = False\n",
    "\n",
    "# Select a test datasets (on axis/off axis)\n",
    "args['opts'] = [\n",
    "    'OUTPUT_DIR', '/mnt/storage-lite/experiments/cta/angular/smooth_experiments/20220604_072240_umonne_lst_full_l21000',\n",
    "    'DATASETS.TEST.FOLDER', '/mnt/storage-lite/datasets/cta/Prod5_DL1/gamma-diffuse/test',\n",
    "    'DATASETS.TEST.EVENTS', '/mnt/storage-lite/datasets/cta/Prod5_DL1/DL1_Prod5_GammaDiffuse_Test/cut_hillas_intensity_1000/events',\n",
    "    'DATASETS.TEST.TELESCOPES', '/mnt/storage-lite/datasets/cta/Prod5_DL1/DL1_Prod5_GammaDiffuse_Test/cut_hillas_intensity_1000/telescopes'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8ba8b5-7136-41e6-a8d0-fed3d257eee8",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b8be2-bb60-4679-88f5-cf773cdf83bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the config.yml\n",
    "if os.path.isdir(args.config_file):\n",
    "    args.config_file = os.path.join(args.config_file, 'config.yml')\n",
    "# Load the configurations\n",
    "cfg = setup_cfg(args)\n",
    "output_dir, evaluation_dir = setup_experiment(cfg, training=False)\n",
    "logger = setup_environment(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b02df3-9f29-4fdb-8542-87d68be8ae2d",
   "metadata": {},
   "source": [
    "## Load evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3c99f7-9c0f-4b93-b852-ff839ac5a3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup evaluation datasets directory\n",
    "if args.use_validation:\n",
    "    evaluation_dataset_name = 'validation'\n",
    "else:\n",
    "    evaluation_dataset_name = 'test'\n",
    "evaluation_dir = evaluation_dir / evaluation_dataset_name\n",
    "evaluation_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Build evaluation dataset\n",
    "evaluation_dataset = build_dataset(cfg, evaluation_dataset_name)\n",
    "describe_dataset(evaluation_dataset, logger, save_to=evaluation_dir / 'description.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eb5966",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31018d8-563b-42a2-8192-cb4cf469ac9d",
   "metadata": {},
   "source": [
    "## Build generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b5bb8-2241-48c3-8acb-2d3cd1508b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_generator = build_generator(cfg, evaluation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4d63e8-781e-4e59-bae3-2275fa283c0b",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6057160b-c459-4016-a964-1ef544f0cdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "input_shape = evaluation_generator.get_input_shape()\n",
    "model = build_model(cfg, input_shape)\n",
    "model.summary()\n",
    "model = load_model(model, evaluation_generator, output_dir, args.epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdad409a-296c-4d9b-89e5-a44117382be1",
   "metadata": {},
   "source": [
    "## Start evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d30d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "random_state = np.random.RandomState(18061996)\n",
    "batch_samples = random_state.randint(len(evaluation_generator) - 1, size=n_samples)\n",
    "batch_samples = [3, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d262bd-92f3-41e7-aa0f-41b285f7675a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "events = []\n",
    "uncertainties = []\n",
    "for i, (X, event_true) in enumerate(tqdm(evaluation_generator)):\n",
    "    predictions, y, uncertainty = model(X, uncertainty=True)\n",
    "    event_predictions = Event.add_prediction_list(event_true, predictions, model.task)\n",
    "    events += event_predictions\n",
    "    uncertainties += [u for u in uncertainty.numpy()]\n",
    "    if (model.task is Task.REGRESSION) and (i in batch_samples):\n",
    "        j = random_state.randint(len(X))\n",
    "        targets = cfg.OUTPUT.REGRESSION.TARGETS\n",
    "        targets_domains = cfg.OUTPUT.REGRESSION.TARGETS_DOMAINS\n",
    "        # Plot input\n",
    "        input_observation = X[j]\n",
    "        # Plot event prediction\n",
    "        event_prediction = event_predictions[j]\n",
    "        model_output = y[j]\n",
    "        event_regression(event_prediction, model_output, model.REGRESSION_OUTPUT_TYPE, targets, targets_domains)\n",
    "evaluation_results = Event.list_to_dataframe(events)\n",
    "evaluation_results['uncertainty'] = uncertainties\n",
    "evaluation_results.to_csv(evaluation_dir / 'results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986ade05",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1480eafb",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9310b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gerumo.visualization import metrics\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be284e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = pd.read_csv(evaluation_dir / 'results.csv')\n",
    "evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d32235",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if model.task is Task.REGRESSION:\n",
    "    # Target Regression\n",
    "    targets = [t.split('_')[1] for t in cfg.OUTPUT.REGRESSION.TARGETS]\n",
    "    metrics.targets_regression(evaluation_results, targets)\n",
    "    # Resolution\n",
    "    metrics.reconstruction_resolution(evaluation_results, targets, ylim=(0, 2))\n",
    "    # Theta2 distribution\n",
    "    metrics.theta2_distribution(evaluation_results, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1518f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model.task is Task.CLASSIFICATION:\n",
    "    # Classification Report\n",
    "    labels = evaluation_generator.output_mapper.classes\n",
    "    metrics.classification_report(evaluation_results.pred_class_id, evaluation_results.true_class_id, labels=labels)\n",
    "    metrics.confusion_matrix(evaluation_results.pred_class_id, evaluation_results.true_class_id, labels=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d76492",
   "metadata": {},
   "source": [
    "# Sample Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be54264",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d28b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select random batch\n",
    "batch_i = np.random.randint(len(evaluation_generator))\n",
    "X, event_true = evaluation_generator[batch_i]\n",
    "\n",
    "# Prediction\n",
    "predictions, y, uncertainties = model(X, uncertainty=True)\n",
    "event_predictions = Event.add_prediction_list(event_true, predictions, model.task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05f9a7f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "samples_j = np.random.randint(len(X), size=n_samples)\n",
    "if model.task is Task.REGRESSION:\n",
    "    targets = cfg.OUTPUT.REGRESSION.TARGETS\n",
    "    targets_domains = cfg.OUTPUT.REGRESSION.TARGETS_DOMAINS\n",
    "    for j in samples_j:\n",
    "        # Plot input\n",
    "        input_observation = X[j]\n",
    "        # Plot event prediction\n",
    "        event_prediction = event_predictions[j]\n",
    "        model_output = y[j]\n",
    "        event_regression(event_prediction, model_output, model.REGRESSION_OUTPUT_TYPE, targets, targets_domains)\n",
    "        # Plot uncertainty\n",
    "        uncertainty = uncertainties[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c75895",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model.task is Task.CLASSIFICATION:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ef67963dbb4be9d51d0b8f2d4d59e748d67c8cc5b776d08e4e371277ec333e32"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
